{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código irá ler a tabela na pasta e processar para uma outra pasta. Dessa forma sempre atualizando a nova tabela apenas com itens novos.\n",
    "Depois aqui mesmo, será feito uma separação da base para levar a base para o PostgreSQL.\n",
    "As bases serão 5:\n",
    "- dimCustomer\n",
    "- dimManager\n",
    "- dimProduct\n",
    "- dimStore\n",
    "- factOrders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import uuid\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um script para processar os dados\n",
    "Esse script vai ler a tabela na fonte e salvar em uma pasta chamada **processado**, dessa forma trazendo apenas dados novos e mantendo os que já estão dentro da pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para a pasta onde o arquivo Excel está localizado\n",
    "source_file_path = 'original\\\\stage_MultiStore.xlsx'\n",
    "# Caminho para a pasta onde o arquivo processado será movido\n",
    "processed_folder = 'processado'\n",
    "# Caminho para o arquivo de tabela mestre\n",
    "master_file_path = 'processado\\\\tabela_mestre.xlsx'\n",
    "\n",
    "# Criar a pasta processado se ela não existir\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "# Função para processar o arquivo Excel\n",
    "def process_excel_file():\n",
    "    # Ler o arquivo Excel original\n",
    "    df_new = pd.read_excel(source_file_path)\n",
    "    \n",
    "    # Verificar se a tabela mestre existe\n",
    "    if os.path.exists(master_file_path):\n",
    "        # Ler o arquivo mestre existente\n",
    "        master_df = pd.read_excel(master_file_path)\n",
    "        # Identificar novas linhas baseadas na coluna 'id'\n",
    "        new_rows = df_new[~df_new['Order ID'].isin(master_df['Order ID'])]\n",
    "        # Adicionar as novas linhas ao arquivo mestre\n",
    "        updated_master_df = pd.concat([master_df, new_rows], ignore_index=True)\n",
    "    else:\n",
    "        # Se não existir, o novo arquivo é o arquivo mestre\n",
    "        updated_master_df = df_new\n",
    "    \n",
    "    # Salvar o arquivo mestre atualizado\n",
    "    updated_master_df.to_excel(master_file_path, index=False)\n",
    "\n",
    "# Executar a função\n",
    "process_excel_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando as bases e criando um arquivo csv para cada tabela nova\n",
    "Essa tabela será enviada para o PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = pd.read_excel(\"processado/tabela_mestre.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_customer[['Customer ID', 'Customer Name', 'Customer State']]\n",
    "\n",
    "# Agrupar por 'Custumer ID' e manter apenas as colunas selecionadas\n",
    "dimCustomer = df_selected.groupby('Customer ID').first().reset_index()\n",
    "\n",
    "# Renomear as colunas\n",
    "dimCustomer.columns = ['customerId', 'customerName', 'isActive']\n",
    "\n",
    "dimCustomer['isActive'] = dimCustomer['isActive'].apply(lambda x: True if x == 'Active' else False)\n",
    "\n",
    "# # Salvar o DataFrame atualizado\n",
    "# df_grouped.to_excel('processado/agrupado_tabela_mestre.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em formato de csv em uma pasta separada o dimCustomer \n",
    "\n",
    "store_split_folder = 'processado/store_split'\n",
    "\n",
    "csv_file_path = os.path.join(store_split_folder, 'dim_customer.csv')\n",
    "\n",
    "os.makedirs(store_split_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "dimCustomer.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manager = pd.read_excel(\"processado/tabela_mestre.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_customer[['Regional Manager ID', 'Regional Manager']]\n",
    "\n",
    "# Agrupar por 'Custumer ID' e manter apenas as colunas selecionadas\n",
    "dimManager = df_selected.groupby('Regional Manager ID').first().reset_index()\n",
    "\n",
    "# Renomear as colunas\n",
    "dimManager.columns = ['managerId', 'managerName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em formato de csv em uma pasta separada o dimManager \n",
    "\n",
    "store_split_folder = 'processado/store_split'\n",
    "\n",
    "csv_file_path = os.path.join(store_split_folder, 'dim_manager.csv')\n",
    "\n",
    "os.makedirs(store_split_folder, exist_ok=True)\n",
    "\n",
    "dimManager.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dim Product\n",
    "Tabela com os produtos - Não contem preço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product = pd.read_excel(\"processado/tabela_mestre.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_product[['Product ID', 'Category', 'Sub-Category', 'Product Name']]\n",
    "\n",
    "# Agrupar por 'Custumer ID' e manter apenas as colunas selecionadas\n",
    "dimProduct = df_selected.groupby('Product ID').first().reset_index()\n",
    "\n",
    "# Renomear as colunas\n",
    "dimProduct.columns = ['productId', 'category', 'subCategory', 'productName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em formato de csv em uma pasta separada o dimManager \n",
    "\n",
    "store_split_folder = 'processado/store_split'\n",
    "\n",
    "csv_file_path = os.path.join(store_split_folder, 'dim_product.csv')\n",
    "\n",
    "os.makedirs(store_split_folder, exist_ok=True)\n",
    "\n",
    "dimProduct.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim Store\n",
    "Informações referente a localização da loja apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store = pd.read_excel(\"processado/tabela_mestre.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar as colunas desejadas\n",
    "df_selected = df_store[['Country', 'State', 'City', 'Region', 'Postal Code']]\n",
    "\n",
    "\n",
    "# # Remover duplicatas para garantir que cada combinação seja única\n",
    "df_store = df_selected.drop_duplicates()\n",
    "\n",
    "# Definir o prefixo e o ano atual\n",
    "prefix = 'STO'\n",
    "\n",
    "# Função para gerar um hash único baseado nos valores das colunas\n",
    "def generate_unique_id(row):\n",
    "    hash_input = f\"{row['Country']}_{row['State']}_{row['City']}_{row['Region']}_{row['Postal Code']}\"\n",
    "    # Gerar um hash MD5 da string e garantir que o ID é único\n",
    "    return f\"{prefix}-{hashlib.md5(hash_input.encode()).hexdigest()[:6].upper()}\"\n",
    "\n",
    "# Adicionar a coluna de ID gerado com hash\n",
    "df_store['storeId'] = df_store.apply(generate_unique_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em formato de csv em uma pasta separada o dimManager \n",
    "\n",
    "store_split_folder = 'processado/store_split'\n",
    "\n",
    "csv_file_path = os.path.join(store_split_folder, 'dim_store.csv')\n",
    "\n",
    "os.makedirs(store_split_folder, exist_ok=True)\n",
    "\n",
    "df_store.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.read_excel(\"processado/tabela_mestre.xlsx\")\n",
    "\n",
    "# Join para trazer storeId\n",
    "df_ordersJoin = pd.merge(df_orders, df_store, \n",
    "                     on=['Country', 'State', 'City', 'Region','Postal Code'], \n",
    "                     how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_17692\\917624760.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fact_orders['Order Date'] = fact_orders['Order Date'].apply(excel_serial_to_datetime)\n",
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_17692\\917624760.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fact_orders['Ship Date'] = fact_orders['Ship Date'].apply(excel_serial_to_datetime)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fact_orders = df_ordersJoin[['Order ID',\n",
    "                             'storeId',\n",
    "                             'Order Date',\n",
    "                             'Ship Date',\n",
    "                             'Ship Mode',\n",
    "                             'Customer ID',\n",
    "                             'Segment',\n",
    "                             'Regional Manager ID',\n",
    "                             'Product ID',\n",
    "                             'Sales',\n",
    "                             'Quantity',\n",
    "                             'Discount',\n",
    "                             'Profit']]\n",
    "\n",
    "# Converter números do formato serial do Excel para datetime\n",
    "def excel_serial_to_datetime(serial):\n",
    "    # Excel usa 1º de janeiro de 1900 como o dia 1\n",
    "    return pd.Timestamp('1900-01-01') + pd.to_timedelta(serial - 2, unit='D')\n",
    "\n",
    "fact_orders['Order Date'] = fact_orders['Order Date'].apply(excel_serial_to_datetime)\n",
    "fact_orders['Ship Date'] = fact_orders['Ship Date'].apply(excel_serial_to_datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_17692\\472989207.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fact_orders.rename(columns=renaming_dict, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "renaming_dict = {\n",
    "    'Order ID': 'orderId',\n",
    "    'storeId': 'storeId',\n",
    "    'Order Date': 'orderDate',\n",
    "    'Ship Date': 'shipDate',\n",
    "    'Ship Mode': 'shipMode',\n",
    "    'Customer ID': 'customerId',\n",
    "    'Segment': 'segment',\n",
    "    'Regional Manager ID': 'managerId',\n",
    "    'Product ID': 'productId',\n",
    "    'Sales': 'salePrice',\n",
    "    'Quantity': 'quantity',\n",
    "    'Discount': 'discount',\n",
    "    'Profit': 'profit'\n",
    "}\n",
    "\n",
    "# Renomear as colunas\n",
    "fact_orders.rename(columns=renaming_dict, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em formato de csv em uma pasta separada o dimManager \n",
    "\n",
    "store_split_folder = 'processado/store_split'\n",
    "\n",
    "csv_file_path = os.path.join(store_split_folder, 'fact_orders.csv')\n",
    "\n",
    "os.makedirs(store_split_folder, exist_ok=True)\n",
    "\n",
    "fact_orders.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabelas prontas. Próximo passo é mandar todas essas tabelas para o banco de dados local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrando o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: pandas in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\andre\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\andre\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2\n",
    "!pip install pandas openpyxl psycopg2-binary\n",
    "!pip install --upgrade psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os parâmetros de conexão com o banco de dados\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',  # Banco de dados padrão para criar o novo banco\n",
    "    'user': 'postgres',\n",
    "    'password': 'andre123'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the PostgreSQL server\n",
    "conn = psycopg2.connect(\n",
    "    host=db_params['host'],\n",
    "    database=db_params['database'],\n",
    "    user=db_params['user'],\n",
    "    password=db_params['password']\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Set automatic commit to be true, so that each action is committed without having to call conn.committ() after each command\n",
    "conn.set_session(autocommit=True)\n",
    "\n",
    "# Create the 'soccer' database\n",
    "cur.execute(\"CREATE DATABASE stage\")\n",
    "\n",
    "# Commit the changes and close the connection to the default database\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of 'dim_customer' CSV file:\n",
      "  customerId  customerName  isActive\n",
      "0   AA-10315    Alex Avila      True\n",
      "1   AA-10375  Allen Armold      True\n",
      "\n",
      "\n",
      "Contents of 'dim_manager' CSV file:\n",
      "         managerId       managerName\n",
      "0  EMP-2016-151022  Dianna Wasserman\n",
      "1  EMP-2016-151048    Charles Kaydos\n",
      "\n",
      "\n",
      "Contents of 'dim_store' CSV file:\n",
      "         Country       State         City Region  Postal Code     storeId\n",
      "0  United States    Kentucky    Henderson  South        42420  STO-68D97E\n",
      "1  United States  California  Los Angeles   West        90036  STO-4D7F2D\n",
      "\n",
      "\n",
      "Contents of 'dim_product' CSV file:\n",
      "         productId   category subCategory  \\\n",
      "0  FUR-BO-10000112  Furniture   Bookcases   \n",
      "1  FUR-BO-10000330  Furniture   Bookcases   \n",
      "\n",
      "                                         productName  \n",
      "0   Bush Birmingham Collection Bookcase, Dark Cherry  \n",
      "1  Sauder Camden County Barrister Bookcase, Plank...  \n",
      "\n",
      "\n",
      "Contents of 'fact_orders' CSV file:\n",
      "          orderId     storeId   orderDate    shipDate      shipMode  \\\n",
      "0  CA-2016-152156  STO-68D97E  2016-11-08  2016-11-11  Second Class   \n",
      "1  CA-2016-152156  STO-68D97E  2016-11-08  2016-11-11  Second Class   \n",
      "\n",
      "  customerId   segment        managerId        productId  salePrice  quantity  \\\n",
      "0   CG-12520  Consumer  EMP-2016-152047  FUR-BO-10001798     261.96         2   \n",
      "1   CG-12520  Consumer  EMP-2016-152047  FUR-CH-10000454     731.94         3   \n",
      "\n",
      "   discount    profit  \n",
      "0       0.0   41.9136  \n",
      "1       0.0  219.5820  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect to the 'stage' database\n",
    "db_params['database'] = 'stage'\n",
    "engine = create_engine(f'postgresql://{db_params[\"user\"]}:{db_params[\"password\"]}@{db_params[\"host\"]}/{db_params[\"database\"]}')\n",
    "\n",
    "# Define the file paths for your CSV files\n",
    "csv_files = {\n",
    "    'dim_customer': 'processado/store_split/dim_customer.csv',\n",
    "    'dim_manager': 'processado/store_split/dim_manager.csv',\n",
    "    'dim_store': 'processado/store_split/dim_store.csv',\n",
    "    'dim_product': 'processado/store_split/dim_product.csv',\n",
    "    'fact_orders': 'processado/store_split/fact_orders.csv'\n",
    "}\n",
    "\n",
    "# Load and display the contents of each CSV file to check\n",
    "for table_name, file_path in csv_files.items():\n",
    "    print(f\"Contents of '{table_name}' CSV file:\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.head(2))  # Display the first few rows of the DataFrame\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the CSV files and import them into PostgreSQL\n",
    "for table_name, file_path in csv_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
